import{_ as l,M as d,p as c,q as h,R as t,N as a,V as s,t as e,a1 as n}from"./framework-5866ffd3.js";const u="/assets/flowchart-98c70a61.png",p="/assets/pacer-e6c4c498.png",f="/assets/clayton-b3635183.png",g="/assets/uft-5faa0e27.png",m="/assets/mongo-39faaa21.png",y="/assets/livingreports-cccaefc6.png",b="/assets/docsite-5cde0c70.png",_="/assets/fileserver-9af1fe06.png",w="/assets/postgres-d2f8c5c6.png",v="/assets/satyrn-58aa44d0.png",k={},S=t("h1",{id:"notes-on-our-internal-data-pipeline",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#notes-on-our-internal-data-pipeline","aria-hidden":"true"},"#"),e(" Notes on our internal data pipeline")],-1),x={style:{"margin-top":"-0.5em"}},A=t("em",null,"by Scott Daniel • last updated Mar 26, 2024",-1),C=n('<h2 id="overview" tabindex="-1"><a class="header-anchor" href="#overview" aria-hidden="true">#</a> Overview</h2><p>To create SCALES data files and furnish data to SCALES apps, we scrape court data from publicly available sources and apply a series of transformations to those data before making them available at public endpoints. In its most abstract form, this data pipeline is structured as follows:</p><div style="text-align:center;"><img src="'+u+'"></div><p>Elsewhere on this website, we describe various SCALES <em>products</em>: aggregate data files, public versions of the inference algorithms contributing to those files, and software tools we developed while working on those files. This document, however, describes the underlying data-manipulation <em>process</em> that produced those files. It&#39;s both a guide for users curious about SCALES&#39;s behind-the-scenes data practices and a reference for SCALES developers themselves.</p><h4 id="general-development-maintenance-needed" tabindex="-1"><a class="header-anchor" href="#general-development-maintenance-needed" aria-hidden="true">#</a> General development &amp; maintenance needed</h4>',5),P=t("li",null,'All data from the state courts of Clayton County are currently confined to the "primary data preparation" layer, and major changes to the subsequent layers will be needed in order for the Clayton data to flow onwards. Additionally, we are in the process of expanding our data holdings to state courts beyond Clayton, and given that the courts at this level vary greatly in their record-keeping practices, many of the components below may need to be modified to accommodate the shapes of the new data.',-1),T=t("li",null,"At present, we're sending data through the pipeline in large, infrequent batches. If a time ever comes when we need to execute some or all of the pipeline with more regularity, many of the components below may need to be modified to be more easily triggerable through Airflow (which we've used in the past, mianly for scraping runs) or some other workflow-management solution.",-1),E={href:"https://wordpress.com/domains/manage/scales-okn.org/dns/scales-okn.org",target:"_blank",rel:"noopener noreferrer"},L=t("code",null,"livingreports",-1),R=t("code",null,"docs",-1),D=t("code",null,"satyrn",-1),M=t("code",null,"satyrn-qa",-1),N=t("h2",{id:"data-sources",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#data-sources","aria-hidden":"true"},"#"),e(" Data sources")],-1),I=t("p",null,"Everything that flows through our pipeline originates from just a handful of locations:",-1),F=t("h3",{id:"federal-courts-pacer",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#federal-courts-pacer","aria-hidden":"true"},"#"),e(" Federal courts (PACER)")],-1),q={href:"https://pacer.uscourts.gov/",target:"_blank",rel:"noopener noreferrer"},j={href:"https://fixthecourt.com/2022/12/the-free-pacer-bill-is-not-becoming-law-this-year-but-thats-not-the-entirety-nor-the-end-of-the-story/",target:"_blank",rel:"noopener noreferrer"},J=n('<div style="text-align:center;"><img src="'+p+'" style="width:90%;"></div><p>Out of the 1,278,268 total PACER cases in our dataset, we scraped 1,181,616¹ cases ourselves, during the following intervals (&quot;cross-sectional&quot; refers to data across all courts):</p><ul><li>1/1/19-1/10/19 (and a few on 7/29/21): <strong>longitudinal data in N.D. Ga</strong> (2010 &amp; 2015; 11,398 cases)</li><li>12/9/18–10/25/23, multiple runs²: <strong>longitudinal data in N.D. Ill</strong> (2002 – Mar &#39;21; 199,664 cases)</li><li>11/14/17–10/25/23, mostly Jan-Feb &#39;20: <strong>cross-sectional data for &#39;16, non-N.D.-Ill</strong> (247,416 cases)</li><li>1/31/20–10/25/23, mostly Jul &#39;21: <strong>cross-sectional data for &#39;17, non-N.D.-Ill</strong> (319,718 cases)</li><li>6/22/21–5/3/23: <strong>misc data in 14 courts</strong>³ (new filings; 4694 cases; <em>initial docket entries only</em>)</li><li>2/8/22–3/14/22: <strong>longitudinal data in E.D. Pa</strong> (2002-21; 326,102 cases; <em>private use only</em>)</li><li>8/2/22–8/26/22: <strong>longitudinal data in TX</strong> (2018 &amp; 2020; 72,349 cases)</li></ul>',3),O={style:{"margin-top":"-0.5em","margin-bottom":"1.5em"}},H=t("br",null,null,-1),W=t("br",null,null,-1),B=t("em",null,"azd, cacd, cand, cod, dcd, ilnd, ksd, kyed, kywd, laed, mad, mdd, njd, nywd.",-1),U=n('<p>The remaining 96,652 cases are those we&#39;ve received via data dumps of PACER HTMLs from third parties, which were originally scraped by those third parties during the following intervals:</p><ul><li>11/14/17–8/19/21, mostly Jun &#39;19: <strong>misc data for &#39;16</strong> (86,169 cases; Free Law Project)</li><li>1/10/23–1/21/23: <strong>misc data in AL, GA, &amp; IL</strong> (10,483 cases; Civil Rights Litigation Clearinghouse)</li></ul><p>Finally, as a part of various projects, we&#39;ve revisited some cases we already scraped in order to pull any new docket entries that may have appeared since our first download. The resulting HTML files — which the scraper code refers to as &quot;case updates&quot; — are fragmentary, in that their docket tables have been run through PACER&#39;s date filter and contain only the new entries (although the HTML fragments are re-unified into single JSON files later on, during the <a href="#how-are-they-produced">parsing step</a> of the pipeline). To distinguish case-update files from the original files, the scraper uses the name <code>12345_1.html</code> for the first update to case <code>12345.html</code>, and the index after the underscore increases for subsequent updates. We&#39;ve downloaded 19,492 case-update files, but because the cases in these files are already represented elsewhere in the corpus, they are not included in the case totals above.</p>',3),z=t("a",{href:"#file-server"},"file server",-1),G=t("h3",{id:"state-courts",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#state-courts","aria-hidden":"true"},"#"),e(" State courts")],-1),Q={href:"https://weba.claytoncountyga.gov/casinqsvr/htdocs/index.shtml",target:"_blank",rel:"noopener noreferrer"},V=n('<div style="text-align:center;"><img src="'+f+'"></div><p>We&#39;ve scraped a total of 3,207,790 Clayton County cases during the following intervals (&quot;n&gt;100&quot; refers to the first year with more than 100 cases):</p><ul><li>10/24/22–11/11/22: <strong>all criminal data in superior court</strong> (108,546 cases; from 1973; n&gt;100 fr 1984)</li><li>11/22/22–1/14/23: <strong>all criminal data in magistrate court</strong> (538,845 cases; from 1985; n&gt;100 fr 1994)</li><li>1/18/23–3/28/23: <strong>all criminal data in state court</strong> (1,483,795 cases; from 1967; n&gt;100 from 1981)</li><li>10/3/23–11/7/23: <strong>all civil data in superior court</strong> (112,245 cases; from 1968; n&gt;100 from 1989)</li><li>11/8/23–1/3/24: <strong>all civil data in magistrate court</strong> (771,957 cases; from 1987; n&gt;100 from 1991)</li><li>3/25/24-4/7/24: <strong>all civil data in state court</strong> (192,402 cases; from 1982; n&gt;100 from 1989)</li></ul><h3 id="minor-data-sources" tabindex="-1"><a class="header-anchor" href="#minor-data-sources" aria-hidden="true">#</a> Minor data sources</h3><p>The following data sources don&#39;t play a major role in our data work, but they&#39;re nevertheless passed along the pipeline in one form or another.</p><h4 id="fjc-idb" tabindex="-1"><a class="header-anchor" href="#fjc-idb" aria-hidden="true">#</a> FJC IDB</h4>',6),X={href:"https://www.fjc.gov/research/idb",target:"_blank",rel:"noopener noreferrer"},K=t("h4",{id:"justfair",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#justfair","aria-hidden":"true"},"#"),e(" JUSTFAIR")],-1),Y={href:"https://qsideinstitute.org/research/criminal-justice/justfair/",target:"_blank",rel:"noopener noreferrer"},Z=n('<h2 id="primary-data-preparation" tabindex="-1"><a class="header-anchor" href="#primary-data-preparation" aria-hidden="true">#</a> Primary data preparation</h2><h3 id="flat-files" tabindex="-1"><a class="header-anchor" href="#flat-files" aria-hidden="true">#</a> Flat files</h3><h4 id="what-are-they" tabindex="-1"><a class="header-anchor" href="#what-are-they" aria-hidden="true">#</a> <em>What are they?</em></h4>',3),$=t("code",null,"/mnt/datastore",-1),ee=n('<p>This collection of flat files is the ground truth upon which all further SCALES work is based; in other words, the flat files comprise our data lake, or the lowest layer of our data stack. The files are &quot;flat&quot; in that they stand alone; they exist in a relatively unstructured state and aren&#39;t linked together in a database or other such imposed structure.</p><p>The flat files are designed primarily as raw material to be passed further along the pipeline, but they can also be useful in their own right. For users outside the SCALES organization, <a href="../parserguide">this guide</a> suggests some use cases for our PACER JSONs. And for our internal work, we&#39;ve further augmented the flat files&#39; utility via a PACER metadata table at <code>/mnt/datastore/unique_docket_filepaths_table.csv</code> on Delilah, which facilitates direct file reads and simple aggregation (as well as providing a full list of PACER filepaths at a few points in the pipeline, like in the <code>make_ucid_lists.py</code> script used to populate the <a href="#file-server">file server</a>). The following wrapper function provides straightforward access to the table:</p><div style="text-align:center;"><img src="'+g+'" style="width:70%;"></div><h4 id="how-are-they-produced" tabindex="-1"><a class="header-anchor" href="#how-are-they-produced" aria-hidden="true">#</a> <em>How are they produced?</em></h4>',4),te=t("code",null,"/mnt/datastore",-1),ae=t("code",null,"infrastructure_dev/code/downloader",-1),oe=t("code",null,"downloader_county",-1),se=t("code",null,"infrastructure_dev/code/parsers",-1),ne=t("code",null,"parsers_county",-1),ie=t("a",{href:"../../scraper"},"scraper",-1),re=t("a",{href:"../../parser"},"parser",-1),de=t("code",null,"infrastructure_dev/code/parsers/schemas",-1),le={href:"https://github.com/scales-okn/PACER-tools",target:"_blank",rel:"noopener noreferrer"},ce=t("code",null,"infrastructure/code/sync_public.py",-1),he={href:"https://pypi.org/project/pacer-tools/",target:"_blank",rel:"noopener noreferrer"},ue=t("code",null,"PACER-tools/setup.py",-1),pe=n('<p>We use the workflow-management platform Airflow to schedule our scraping runs outside of working hours (in order to avoid overtaxing court servers) and to facilitate any runs large enough that they need to be spread across multiple days. Airflow jobs execute in a Docker container on Delilah, from which they communicate via ssh with a separate container dedicated to running <code>infrastructure_dev</code> scripts. The code for this system, as well as a readme with more information, can be found in <code>infrastructure_dev/code/apps/conductor</code>.</p><p>As for the minor data sources in our datastore, we used the scripts at <code>infrastructure_dev/code/tasks/fjc_*.py</code> for all data munging related to the FJC IDB, and we used the notebook at <code>infrastructure_dev/code/experiments/justfair/playground.ipynb</code> to produce the one JUSTFAIR file not taken right from the study (<code>/mnt/datastore/justfair/ucid_dataset.csv</code>). Finally, to maintain the PACER metadata table, we use the scripts <code>build_unique_table.py</code> and <code>update_uft.py</code> in <code>infrastructure_dev/code/tasks</code>.</p><h4 id="development-maintenance-needed" tabindex="-1"><a class="header-anchor" href="#development-maintenance-needed" aria-hidden="true">#</a> <em>Development &amp; maintenance needed</em></h4>',3),fe=t("li",null,[e("On various occasions, we've experimented with downloading some of the documents that are linked from specific PACER docket lines, which are usually PDF copies of individual filings by litigants; we've stored these documents parallel to the HTML & JSON files in "),t("code",null,"/mnt/datastore/pacer"),e(" on Delilah. At present, we're not doing anything with the PDFs, but eventually we'd like to pass them further along in the pipeline, either via simple database storage (e.g. Mongo GridFS) or more sophisticated OCR work.")],-1),ge=t("li",null,'No schema files currently exist describing the ways in which the structure of the state court JSON files diverge from the structure of the PACER JSONs (see e.g. the new fields in the Clayton parser marked with the comment "NEW FIELD"). Additionally, since the last update to the PACER schema files, revisions to the PACER parser may have resulted in some small changes to the structure of the PACER JSONs.',-1),me=n('<h3 id="annotations" tabindex="-1"><a class="header-anchor" href="#annotations" aria-hidden="true">#</a> Annotations</h3><h4 id="what-are-they-1" tabindex="-1"><a class="header-anchor" href="#what-are-they-1" aria-hidden="true">#</a> <em>What are they?</em></h4><p>We use the umbrella term &quot;annotation&quot; to refer to a piece of data produced by some kind of inference about the flat files. For PACER cases, we&#39;ve produced the following categories of annotations, all stored in <code>/mnt/datastore/</code>:</p><ul><li><strong>Judges</strong>: <code>judge_disambiguation/</code></li><li><strong>Parties</strong>: <code>party_disambiguation/</code></li><li><strong>Lawyers</strong>: <code>counsel_disambiguation/</code></li><li><strong>Law firms</strong>: <code>firm_disambiguation/</code></li><li>Docket-entry tags using SCALES&#39;s <strong>case-event ontology</strong>: <code>ontology/mongo/labels.csv</code></li><li>Docket-entry tags for <strong>in forma pauperis (IFP)</strong> activity: currently stored only on Mongo</li></ul><p>For Clayton County cases, we&#39;ve produced the following annotations (albeit most only for a small selection of courts and years), all stored in <code>/mnt/datastore/annotation/counties/ga_clayton/</code>:</p><ul><li><strong>Lawyers</strong>: <code>lawyers.csv</code>, <code>lawyer_appearances_16_to_18.csv</code></li><li><strong>Prison sentences</strong>: <code>superior_sentences.csv</code></li><li><strong>Bond events</strong>: <code>bonds_16_to_18.csv</code></li><li>ROTA/BJS <strong>charge categories</strong>: <code>clayton_charge_categories_rota_bjs.csv</code></li><li><strong>Crosswalk</strong> from cases in criminal magistrate court to other criminal cases: TBD (in progress)</li></ul><p>Other pages on this site contain more information about the <a href="../disambiguation">entity-disambiguation annotations</a> and <a href="../ontology">ontology annotations</a>.</p><h4 id="how-are-they-produced-1" tabindex="-1"><a class="header-anchor" href="#how-are-they-produced-1" aria-hidden="true">#</a> <em>How are they produced?</em></h4>',8),ye={href:"https://huggingface.co/scales-okn",target:"_blank",rel:"noopener noreferrer"},be={href:"https://github.com/scales-okn/label-studio",target:"_blank",rel:"noopener noreferrer"},_e=t("li",null,[e("Judges: "),t("code",null,"research_dev/code/research/entity_resolution/judges_private/JED_*.py"),e(" (excluding "),t("code",null,"JED_X"),e(" helper scripts)")],-1),we=t("li",null,[e("Lawyers: "),t("code",null,"research_dev/code/research/entity_resolution/run_counsel_pipeline.py")],-1),ve=t("li",null,[e("Law firms: "),t("code",null,"research_dev/code/research/entity_resolution/run_firm_pipeline.py")],-1),ke={href:"https://pypi.org/project/scales-nlp/",target:"_blank",rel:"noopener noreferrer"},Se=t("a",{href:"../../nlp"},"SCALES NLP documentation",-1),xe=t("li",null,[e("IFP tags: "),t("code",null,"research_dev/code/research/ifp/build_judge_ifp_data.py")],-1),Ae=n('<p>(Note that changes to the scales-nlp package can be published to pypi using the same method described in <code>PACER-tools/setup.py</code>.)</p><p>We&#39;ve generated most of the Clayton County annotations on an as-needed basis, using code not yet published to any of our repos. An exception is <code>superior_sentences.csv</code>, which comes from <code>research_dev/code/research/counties/ga_clayton/parse_sentences.py</code>.</p><h4 id="development-maintenance-needed-1" tabindex="-1"><a class="header-anchor" href="#development-maintenance-needed-1" aria-hidden="true">#</a> <em>Development &amp; maintenance needed</em></h4><ul><li>The party-disambiguation pipeline should be fully implemented in parallel with the counsel/firm disambiguation pipelines, as outlined in the documentation for the latter pipelines.</li><li>The ad-hoc Jupyter notebook operations which were used to generate the Clayton County annotations should be moved into scripts.</li><li>An IFP-annotations file should be added to the datastore.</li></ul><h2 id="intermediate-destinations" tabindex="-1"><a class="header-anchor" href="#intermediate-destinations" aria-hidden="true">#</a> Intermediate destinations</h2><h3 id="mongodb-database" tabindex="-1"><a class="header-anchor" href="#mongodb-database" aria-hidden="true">#</a> MongoDB database</h3><h4 id="how-is-it-produced" tabindex="-1"><a class="header-anchor" href="#how-is-it-produced" aria-hidden="true">#</a> <em>How is it produced?</em></h4>',7),Ce=t("code",null,"infrastructure_dev/code/db/mongo/update_collection.py",-1),Pe=t("code",null,"mongo_tools.py",-1),Te=t("code",null,"cases_html",-1),Ee=t("h4",{id:"what-is-it",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#what-is-it","aria-hidden":"true"},"#"),e(),t("em",null,"What is it?")],-1),Le=t("p",null,[e("To obviate the need to look up the syntax associated with "),t("code",null,"pymongo.MongoClient"),e(", we provide a wrapper class called "),t("code",null,"SCALESMongo"),e(" in "),t("code",null,"support/mongo_connector.py"),e(" that can be called as follows (by users with Mongo credentials):")],-1),Re=t("div",{style:{"text-align":"center"}},[t("img",{src:m,style:{width:"90%"}})],-1),De={href:"https://github.com/scales-okn/infrastructure_dev/tree/master/code/db/mongo",target:"_blank",rel:"noopener noreferrer"},Me=n('<h4 id="development-maintenance-needed-2" tabindex="-1"><a class="header-anchor" href="#development-maintenance-needed-2" aria-hidden="true">#</a> <em>Development &amp; maintenance needed</em></h4><ul><li>The Mongo readme is incomplete and slightly out of date (e.g. it dates back to a time when we hoped to use Airflow to automate most of this pipeline).</li><li>During the load to Mongo, we remove social security numbers and prisoner numbers from the case data, using a call written into the loading code to the regex-based <code>data_tools.remove_sensitive_info()</code>. The SSN-removal part of this function is fairly robust, but the detection of prisoner numbers is a bit spotty and could be improved.</li><li>The Mongo-load step of the pipeline is also the point at which we <a href="../../redaction">redact</a> the names of private individuals from the case data. Our current strategy is to run the redaction code on the full PACER dataset (via <code>tasks/redact_pacer.py</code>) to create a <code>pacer_redacted</code> directory, which we then load to Mongo in bulk. This strategy works fine, but if we begin doing Mongo loads on a more piecemeal basis (or if we want to ensure that no one accidentally skips the redaction step when loading to Mongo), then it may be preferable to call the redaction code directly from the loading code, as with the SSN removal.</li></ul><h3 id="living-reports" tabindex="-1"><a class="header-anchor" href="#living-reports" aria-hidden="true">#</a> Living reports</h3><div style="text-align:center;"><img src="'+y+'" style="width:90%;margin-top:1em;"></div><h4 id="how-are-they-produced-2" tabindex="-1"><a class="header-anchor" href="#how-are-they-produced-2" aria-hidden="true">#</a> <em>How are they produced?</em></h4>',5),Ne={href:"http://livingreports.scales-okn.org/",target:"_blank",rel:"noopener noreferrer"},Ie=t("code",null,"/var/www/html/living_reports",-1),Fe={href:"https://github.com/scales-okn/living_reports",target:"_blank",rel:"noopener noreferrer"},qe={href:"https://github.com/scales-okn/living-reports-tutorial",target:"_blank",rel:"noopener noreferrer"},je=n("<p>Specialized data files provide the data used by the reports. We generate these files on Delilah and scp them to <code>living_reports/flask/data</code> on the droplet, as follows:</p><ul><li><strong>In forma pauperis (IFP)</strong>: PACER cases &amp; judge annotations --&gt; <code>research_dev/code/research/ifp/build_judge_ifp_data.py</code> --&gt; <code>data/all_districts_ifpcases_clean.csv</code>, <code>data/judge_var_sig.csv</code>, <code>data/judge_var_sig_lookup.csv</code></li><li><strong>Sealing</strong>: PACER cases --&gt; both scripts in <code>research_dev/code/research/sealing</code> --&gt; <code>data/items_over_motions.csv</code>⁴</li><li><strong>IDB</strong>: PACER cases &amp; IDB --&gt; <code>research_dev/code/research/idb/living_report_idb.ipynb</code> --&gt; <code>data/idb/idb_overall.csv</code>, <code>data/idb/idb_ifp_false_pos.csv</code>, <code>data/idb/idb_ifp_false_neg.csv</code></li><li><strong>Data coverage</strong>: PACER cases --&gt; both notebooks in <code>research_dev/code/research/data_coverage</code> --&gt; <code>state_coverage.csv</code>, <code>district_coverage.csv</code>, <code>courts_geojson.json</code></li></ul>",2),Je={style:{"margin-top":"-0.5em","margin-bottom":"1.5em"}},Oe=t("code",null,"data/case_span.csv",-1),He=n('<p>Finally, the app&#39;s authentication system relies on a basic SQLite user database, which lives on the droplet at <code>living_reports/flask/db/app.db</code>. We manually edit this database using the methods described in the living_reports readme (if using <code>flask shell</code>, make sure to run <code>conda activate flask-lr</code> first).</p><h4 id="what-are-they-2" tabindex="-1"><a class="header-anchor" href="#what-are-they-2" aria-hidden="true">#</a> <em>What are they?</em></h4><p>The living reports are dashboard-style writeups on various topics that we feel may be interesting to our user base. Originally, these reports were an important way for us to highlight SCALES&#39;s data-analysis capabilities; when we inherited the Satyrn app, though, we began to spend much less time maintaining and building out the reports.</p><p>The living reports on sealing, data coverage, and the IDB are publicly accessible, while the IFP reports are only accessible to users we&#39;ve added to the small SQLite user database. (There are 94 IFP reports, one for each federal court district, and each user has access to some subset of those reports.)</p><h4 id="development-maintenance-needed-3" tabindex="-1"><a class="header-anchor" href="#development-maintenance-needed-3" aria-hidden="true">#</a> <em>Development &amp; maintenance needed</em></h4>',5),We=n('<h3 id="documentation-site" tabindex="-1"><a class="header-anchor" href="#documentation-site" aria-hidden="true">#</a> Documentation site</h3><div style="text-align:center;"><img src="'+b+'" style="width:90%;margin-top:1em;"></div><h4 id="how-is-it-produced-1" tabindex="-1"><a class="header-anchor" href="#how-is-it-produced-1" aria-hidden="true">#</a> <em>How is it produced?</em></h4>',3),Be={href:"https://docs.scales-okn.org",target:"_blank",rel:"noopener noreferrer"},Ue=t("code",null,"docsite/deploy.sh",-1),ze={href:"https://github.com/scales-okn/docsite",target:"_blank",rel:"noopener noreferrer"},Ge=n('<p>The site content is mostly free-floating prose that isn&#39;t procedurally generated in any way, similar to the README.md files you might expect to see in git repositories; in other words, the onus is on SCALES developers to augment and update the docsite as SCALES projects evolve. One exception to this rule is the set of readmes that are copied directly from their respective SCALES repos via <code>docsite/sync_readme.py</code>. At present, this set includes just the PACER scraper and parser, but more readmes can be added to the set by editing the <code>pairs</code> tuple in the script.</p><h4 id="what-is-it-1" tabindex="-1"><a class="header-anchor" href="#what-is-it-1" aria-hidden="true">#</a> <em>What is it?</em></h4><p>If the scales-okn GitHub page is the central location where a technically-minded person would go to understand how SCALES products work, the docsite is the equivalent place for a non-technically-minded person. There is some overlap between the two (e.g. the synced readmes mentioned above), but the docsite diverges from the repo readmes in that it hosts some article-like &quot;guides&quot; describing broad swaths of our work, offers some readmes for products that don&#39;t have a dedicated repo (e.g. the data files served by the file server), and excludes some readmes unlikely to be of interest to lay users (e.g. the readmes for the Satryn-related repos).</p><h2 id="final-destinations" tabindex="-1"><a class="header-anchor" href="#final-destinations" aria-hidden="true">#</a> Final destinations</h2><h3 id="file-server" tabindex="-1"><a class="header-anchor" href="#file-server" aria-hidden="true">#</a> File server</h3><div style="text-align:center;"><img src="'+_+'" style="width:90%;margin-top:1em;"></div>',6),Qe={style:{"text-align":"center"}},Ve=t("em",null,"n.b.: The case counts shown in this image are accurate at time of writing.",-1),Xe=t("h4",{id:"how-is-it-produced-2",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#how-is-it-produced-2","aria-hidden":"true"},"#"),e(),t("em",null,"How is it produced?")],-1),Ke={href:"http://scalesokndata.ci.northwestern.edu/",target:"_blank",rel:"noopener noreferrer"},Ye=t("code",null,"infrastructure_dev/code/db/rdf/make_ucid_lists.py",-1),Ze=t("code",null,"make_and_scp_filedumps.sh",-1),$e=t("h4",{id:"what-is-it-2",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#what-is-it-2","aria-hidden":"true"},"#"),e(),t("em",null,"What is it?")],-1),et=t("p",null,"The file server is a data-sharing solution for anyone who prefers to work with copies of the flat JSON files living on our system rather than querying a database, such as users who need fast or streaming access to large volumes of our data and don't mind writing custom code in the process. For various reasons — e.g. the scope of researchers' typical data needs, and our own desire not to update the gzips every time we acquire new data — the two kinds of file collections on the server (longitudinal and cross-sectional) represent a limited, albeit extensive, portion of our total data holdings.",-1),tt={href:"https://new.nsf.gov/funding/initiatives/convergence-accelerator",target:"_blank",rel:"noopener noreferrer"},at=t("p",null,"At time of writing, however, the TTLs have recently found a practical application. An infrastructure-focused Convergence Accelerator team is building an API endpoint that will allow all other teams to execute SPARQL queries against a graph database, and we plan on granting file-server access to this infrastructure team so they can incorporate the TTL file dumps into their database. Our long-term goal is to translate Satyrn's under-the-hood SQL queries to SPARQL, allowing Satyrn users to interact with the entirety of the NSF's cross-disciplinary graph database; if these efforts are successful, the importance of the file server and its contents will increase significantly.",-1),ot=t("a",{href:"#federal-courts-pacer"},"earlier",-1),st=n('<p>The <a href="../../rdf">data-files documentation page</a> contains more information on both the JSONs and TTLs, as well as usage examples.</p><h4 id="development-maintenance-needed-4" tabindex="-1"><a class="header-anchor" href="#development-maintenance-needed-4" aria-hidden="true">#</a> <em>Development &amp; maintenance needed</em></h4><ul><li>The case lists are currently maintained manually; if PACER downloads ever pick up speed again, it may be useful to automate the generation of those lists.</li></ul><h3 id="postgresql-database" tabindex="-1"><a class="header-anchor" href="#postgresql-database" aria-hidden="true">#</a> PostgreSQL database</h3><h4 id="what-is-it-3" tabindex="-1"><a class="header-anchor" href="#what-is-it-3" aria-hidden="true">#</a> <em>What is it?</em></h4>',5),nt=n('<p>Users with credentials can access the Postgres database as follows:</p><div style="text-align:center;"><img src="'+w+'" style="width:90%;"></div><h4 id="how-is-it-produced-3" tabindex="-1"><a class="header-anchor" href="#how-is-it-produced-3" aria-hidden="true">#</a> <em>How is it produced?</em></h4><p>To populate the Postgres database, we pass data from Mongo through the ETL sequence in <code>infrastructure_dev/code/db/postgres/scales_mongo_etl.py</code> (which we typically wrap with <code>ucid_mapper.py</code>). For efficiency, we perform a separate bulk load of the ontology annotations, first pulling them from Mongo and staging them in JSONL files in the datastore using <code>scales_ontology_jsonl_writer.py</code> and then loading those files using <code>scales_ontology_jsonl_loader.py</code>.</p><h4 id="development-maintenance-needed-5" tabindex="-1"><a class="header-anchor" href="#development-maintenance-needed-5" aria-hidden="true">#</a> <em>Development &amp; maintenance needed</em></h4><ul><li>The following columns in the Postgres <code>cases</code> table, which were originally added via ad-hoc Jupyter-notebook operations to facilitate certain Satyrn analyses on a tight deadline, need to be written into the main Postgres loader: <code>year</code>, <code>court</code>, <code>state</code>, <code>circuit</code>, <code>sjid</code>, <code>nature_suit</code>, <code>ontology_labels</code>, <code>ifp_decision</code>, <code>pacer_pro_se</code>. Additionally, the entire Postgres <code>judges</code> table was loaded in this way and needs to be incorporated into the loading code as well.</li><li>The Postgres ETL scripts depend on a legacy custom ORM called JumboDB, written many years ago by the <a href="#what-is-it-4">C3 Lab</a>. To avoid code bloat and improve maintainability (e.g. by obviating the need for edits in the JumboDB directory in tandem with any changes to the Postgres schema), it may be worth dispensing with JumboDB if a major refactor of the ETL scripts ever occurs. (The original JumboDB repo is no longer publicly available, but we maintain a SCALES fork of the repo at <code>infrastructure_dev/code/db/postgres/jumbodb</code>.)</li></ul><h3 id="satyrn-app" tabindex="-1"><a class="header-anchor" href="#satyrn-app" aria-hidden="true">#</a> Satyrn app</h3>',7),it=t("code",null,"db_type",-1),rt={href:"https://github.com/scales-okn/satyrn-api/blob/main/core/api/sql_func.py",target:"_blank",rel:"noopener noreferrer"},dt=t("div",{style:{"text-align":"center"}},[t("img",{src:v,style:{width:"90%","margin-top":"1em"}})],-1),lt=t("h4",{id:"how-is-it-produced-4",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#how-is-it-produced-4","aria-hidden":"true"},"#"),e(),t("em",null,"How is it produced?")],-1),ct={href:"https://github.com/scales-okn/scales-ux/",target:"_blank",rel:"noopener noreferrer"},ht={href:"https://github.com/scales-okn/satyrn-api/",target:"_blank",rel:"noopener noreferrer"},ut={href:"https://satyrn.scales-okn.org/",target:"_blank",rel:"noopener noreferrer"},pt=n('<p>In theory, Satyrn is a data-agnostic &quot;dumb&quot; platform, meaning that it relies entirely on the ring file when performing analysis and makes no assumptions about the nature of the underlying data. In practice, though, this agnosticism is hard to enforce fully; thus, the app&#39;s ability to manipulate SCALES data depends on a few specifically tailored sections of code:</p><ul><li><code>views.py</code>/<code>viewHelpers.py</code> (backend): hardcoded function that preps PACER case HTMLs for frontend viewing</li><li><code>extractors.py</code>/<code>seekers.py</code>/<code>views.py</code>/<code>viewHelpers.py</code> (backend): hardcoded table joins during Postgres searches (courts, natures of suit, full-text-search-optimized HTMLs)</li><li><code>autocomplete.py</code> (backend), <code>Filter.tsx</code>/<code>LocalAutocomplete.tsx</code>/<code>autocompleteOptions.tsx</code> (frontend): autocomplete optimizations based on specific Postgres fields</li><li><code>queryBuilder.ts</code> (frontend): minor hardcoded fixes for requests being sent to the backend</li><li><code>BarChartDisplay.tsx</code>/<code>LineChartDisplay.tsx</code>/<code>MultilineChartDisplay.tsx</code> (frontend): minor hardcoded graph fixes</li></ul><p>Finally, as a speed optimization to facilitate text searches over case HTMLs, Postgres consults a full-text index on the Mongo <code>cases_html</code> table (via a connection string in the <code>MONGO_URI</code> environment variable) when filtering Postgres-derived case lists during those searches; accordingly, for completeness, an edge pointing directly from Mongo to Satyrn has been included in the flowchart at the top of this guide. It&#39;s worth noting that the chances of unexpected behavior⁵ stemming from divergence between Mongo &amp; Postgres are slim to none, given that (1) case-HTML changes/deletions are unlikely ever to occur and (2) any such changes would require the Mongo full-text index to be re-created, a lengthy operation that could only conceivably occur in the context of a full-pipeline update.</p><h4 id="what-is-it-4" tabindex="-1"><a class="header-anchor" href="#what-is-it-4" aria-hidden="true">#</a> <em>What is it?</em></h4><p>The Satyrn app is a publicly accessible, no-code, ORM-like research tool styled like a Jupyter notebook. Its main offerings are a variety of filtering, searching, analysis, and plotting operations on PACER data. Kris Hammond&#39;s C3 Lab developed the initial versions of Satyrn in order to explore strategies for translating natural-language questions into SQL-style queries against arbitrary databases. When the C3 Lab handed off development duties to SCALES at the end of 2022, we adapted it into a PACER-data-exploration app more closely tailored to our organization&#39;s goal of making court data accessible to researchers and the lay public.</p>',5),ft=t("a",{href:"mailto:admin@scales-okn.org"},"email us",-1),gt={href:"https://github.com/scottgdaniel",target:"_blank",rel:"noopener noreferrer"},mt={style:{"margin-top":"1.75em"}};function yt(bt,_t){const r=d("font"),i=d("RouterLink"),o=d("ExternalLinkIcon");return c(),h("div",null,[S,t("div",x,[a(r,{size:"2"},{default:s(()=>[A]),_:1})]),C,t("ul",null,[P,T,t("li",null,[e("Some of the components below are tied to Northwestern University (SCALES's former home) and may need to be relocated if NU changes SCALES's access rights; specifically, "),a(i,{to:"/guide/pipeline/#what-are-they"},{default:s(()=>[e("Delilah")]),_:1}),e(" is a physical server located on NU's Evanston campus (and therefore behind the NUIT firewall, meaning an NU NetID is required for SSH access), and our file server is a machine administered by NUIT. Fortunately, SCALES has full control over the rest of the pipeline, via DigitalOcean (Mongo, Postgres, living reports, Satyrn), GitHub Pages (documentation site), and "),t("a",E,[e("WordPress DNS records"),a(o)]),e(" (the "),L,e(", "),R,e(", "),D,e(", and "),M,e(" subdomains of scales-okn.org).")])]),N,I,F,t("p",null,[e("Detailed and uniform across 94 U.S. federal district courts, "),t("a",q,[e("PACER case records"),a(o)]),e(" are a vital tool for legal scholars. In our early days, SCALES focused exclusively on PACER, buying paywalled cases and making them freely available in an easy-to-use format. Even as our interests and technical capabilities have expanded, PACER data has remained a fruitful target for research and thus a continuing focus. (Pending either new partnerships & funding or a revived "),t("a",j,[e('"Free PACER" bill'),a(o)]),e(", we hope to scrape even more PACER data in the future.)")]),J,t("div",O,[a(r,{size:"1"},{default:s(()=>[e("¹ 275 of these cases were scraped accidentally (usually because of filing-date ambiguity in PACER search results) and are not included in the breakdown. "),H,e("² Most ND Ill scraping dates to Dec '18 (2010, 2015), Aug '19 (2007-09, 2011-14, 2016), Aug/Nov '20 (2017-20), Mar '21 (2020-21), & May '21 (2002-06). "),W,e("³ On behalf of NACDL, we scraped new NoS-440 cases in these courts: "),B]),_:1})]),U,t("p",null,[e('The specific cases downloaded during the above runs are listed in the "case lists" folder hosted on our '),z,e("; more information on those case lists can be found "),a(i,{to:"/rdf/#case-lists"},{default:s(()=>[e("here")]),_:1}),e(".")]),G,t("p",null,[e("The sprawling, unstandardized, and inconsistently documented state courts pose myriad challenges to researchers, but given that most U.S. legal activity occurs on the state level, cases in these courts are a crucial piece of the legal-research puzzle. As of this writing, our state-court scraping work is in its pilot phase in the courts of "),t("a",Q,[e("Clayton County, GA"),a(o)]),e(", in partnership with the American Equity & Justice Group and supported by Georgia State University's Project RISE. In the coming year, we also plan to begin scraping data from courts in other counties.")]),V,t("p",null,[e("The Federal Judicial Center's "),t("a",X,[e("Integrated Data Base"),a(o)]),e(" is one of the only free, comprehensive sources of aggregate data about federal court cases, so it is a common starting point for all manner of research into the U.S. judiciary. However, the FJC's data collection and maintenance processes are somewhat opaque, and systematic gaps and errors in IDB data often plague legal scholars' attempts to use it for large-scale research work; therefore, we include the IDB in our Mongo loads and TTL files not as a valued data source but more so as a point of comparison for researchers accustomed to relying on that dataset.")]),K,t("p",null,[e("Created as part of a 2020 "),t("a",Y,[e("study"),a(o)]),e(` entitled "JUSTFAIR: Judicial System Transparency through Federal Archive Inferred Records" (Ciocanel et al.), the JUSTFAIR dataset is an interesting, if imperfect, attempt at crosswalking PACER data with data from the U.S. Sentencing Commission in order to understand individual judges' sentencing decisions. We include it in our Mongo loads mainly as a courtesy for researchers curious about this ongoing area of research.`)]),Z,t("p",null,[e("When we acquire legal data, we deposit those data in their raw form into files on a secure machine accessible only by SCALES developers (hereafter nicknamed Delilah), on a dedicated volume located at "),$,e(". Each file on this volume (excluding the "),a(i,{to:"/guide/pipeline/#annotations"},{default:s(()=>[e("annotations")]),_:1}),e(" subdirectory) is either (1) a direct transcription of some data pulled from one of the "),a(i,{to:"/guide/pipeline/#data-sources"},{default:s(()=>[e("above sources")]),_:1}),e(', like an HTML page from PACER, or (2) a parsed version of one of these direct transcriptions, like a JSON version of an HTML file. (Though it may seem odd to describe both these methods of flat-file production as "depositing data in their raw form," all of our parsing code has been designed to write JSONs that are as "raw" as possible — that is, to provide near-lossless translation from one format to another, where "loss" excludes loss of contextually meaningless information such as, say, inline CSS.)')]),ee,t("p",null,[e("We acquire the data in "),te,e(" by running scraping scripts that capture web pages as HTML files, and subsequently running parsing scripts that convert the HTMLs to cleaner, more structured JSON files. Our scraping libraries reside in "),ae,e(" (PACER) & "),oe,e(" (state courts), and our parsing libraries can be found in "),se,e(" (PACER) & "),ne,e(" (state courts). The documentation pages for the PACER "),ie,e(" and "),re,e(" contain more information about these libraries; additionally, the directory "),de,e(" contains files describing the structure of our PACER JSONs using JSON Schema syntax. (Public versions of the PACER scraping/parsing scripts are available in the "),t("a",le,[e("PACER-tools"),a(o)]),e(" repo, maintained via "),ce,e(", and the "),t("a",he,[e("pacer-tools"),a(o)]),e(" pypi package, maintained via the instructions in "),ue,e(".)")]),pe,t("ul",null,[t("li",null,[e("The unique files table currently tracks only PACER data; eventually, we'll want to extend the table to state-court data (and perhaps even to the "),a(i,{to:"/guide/pipeline/#minor-data-sources"},{default:s(()=>[e("minor data sources")]),_:1}),e(").")]),fe,ge]),me,t("p",null,[e("For most of the PACER annotations, the PACER case data in the Delilah datastore is the only input. The ontology annotations are the sole exception to this rule; in addition to case data, they depend on "),t("a",ye,[e("model weights"),a(o)]),e(" from models trained on hand-tagged docket entries. (A SCALES "),t("a",be,[e("Label Studio instance"),a(o)]),e(" was the arena in which this hand-tagging took place.)")]),t("ul",null,[_e,we,ve,t("li",null,[e("Ontology tags: "),t("a",ke,[e("scales-nlp"),a(o)]),e(" pypi package (see also the "),Se,e(")")]),xe]),Ae,t("p",null,[e("Each load to Mongo begins with the name of a Mongo collection being passed to "),Ce,e(". This script takes care of some high-level tasks like incrementing version numbers before calling "),Pe,e(", which ingests data from either the flat files or the annotations and then performs the actual load. (Note that any loads to the "),Te,e(" collection will now require a costly rewrite of the full-text search index in that collection, as explained in the section on "),a(i,{to:"/guide/pipeline/#satyrn-app"},{default:s(()=>[e("Satyrn")]),_:1}),e(".)")]),Ee,t("p",null,[e("Our Mongo database, hosted on DigitalOcean, is a data-sharing solution for anyone who needs simple table-style access to our data, and/or anyone who doesn't want to go the trouble of downloading and storing the "),a(i,{to:"/guide/pipeline/#file-server"},{default:s(()=>[e("flat-file versions")]),_:1}),e(" of the data (or who needs only a small subset of the files included in those gzips).")]),t("p",null,[e("In the early days of SCALES, this Mongo instance was our only means of disseminating data. Later, when we took over development of the "),a(i,{to:"/guide/pipeline/#satyrn-app"},{default:s(()=>[e("Satyrn app")]),_:1}),e(", we also inherited the Postgres database that the original developers had been using to supply PACER data to the app, which prompted us to consider retiring the Mongo database and using Postgres for all our data-distribution needs. However, we felt it was important to have somewhere to host any data not usable in Satyrn but still useful in other contexts, like the civil-rights "),a(i,{to:"/guide/pipeline/#federal-courts-pacer"},{default:s(()=>[e("stub cases")]),_:1}),e(" we scraped for NACDL. Accordingly, we've limited Postgres to its original role as a data source for Satyrn, while we use Mongo for everything else.")]),Le,Re,t("p",null,[e("More information on both the load process and characteristics of the database itself can be found in the "),t("a",De,[e("readme"),a(o)]),e(".")]),Me,t("p",null,[e("The living-reports app consists of a Vue frontend and Flask backend, running on a droplet at "),t("a",Ne,[e("livingreports.scales-okn.org"),a(o)]),e("; the project directory on that droplet is "),Ie,e(". For info on maintaining the current infrastructure and adding new reports, see the "),t("a",Fe,[e("readme"),a(o)]),e(" for the living_reports repo, as well as this more explicit "),t("a",qe,[e("tutorial"),a(o)]),e(".")]),je,t("div",Je,[a(r,{size:"1"},{default:s(()=>[e("⁴ At present, the code for the sealing report also requires a file called "),Oe,e("; however, the plots that use this file have been removed, so the file no longer needs to be maintained.")]),_:1})]),He,t("ul",null,[t("li",null,[e("For consistency and ease of maintenance, the current authentication system should be swapped out for the Satyrn authentication endpoint; for further explanation, see the section on the "),a(i,{to:"/guide/pipeline/#file-server"},{default:s(()=>[e("file server")]),_:1}),e(", which currently uses that endpoint for authentication.")]),t("li",null,[e("Before doing any major work on the living reports, it's worth considering the possibility that, as the "),a(i,{to:"/guide/pipeline/#satyrn-app"},{default:s(()=>[e("Satyrn app")]),_:1}),e(" improves, it may render future and/or present reports obsolete (or perhaps simply more trouble than they're worth, given the whole separate infrastructure on which they're built).")])]),We,t("p",null,[e("The documentation site (or docsite for short) is a simple Vuepress site, served by GitHub Pages at "),t("a",Be,[e("docs.scales-okn.org"),a(o)]),e(" (which redirects to scales-okn.github.io). Updates to the docsite happen via the "),Ue,e(" script, as documented in the "),t("a",ze,[e("readme"),a(o)]),e(" for the docsite repo.")]),Ge,t("div",Qe,[a(r,{size:"1"},{default:s(()=>[Ve]),_:1})]),Xe,t("p",null,[e("The file server is a Northwestern-administered machine running at "),t("a",Ke,[e("scalesokndata.ci.northwestern.edu"),a(o)]),e(". To copy files to the server (which is only possible via SSH from Delilah, due to the NU firewall), we run "),Ye,e(" to determine the cases that should be included and then run "),Ze,e(" on the resulting case lists. To make these files available to the outside world, we use the server to host a web app that is modeled after the living-reports app and, as such, consists of a Vue frontend & Flask backend; this app is accessible by anyone with Satyrn credentials.")]),$e,et,t("p",null,[e("In addition to JSON file dumps, the file server also hosts files in TTL format (Terse RDF Triple Language), which is intended for graph-database use cases. Our inclusion of TTL files was motivated by the overarching priorities of the NSF's "),t("a",tt,[e("Convergence Accelerator"),a(o)]),e(` program, which has provided the majority of SCALES's funding since our inception. Initially, the TTL files were a proof of concept, demonstrating that court data could be incorporated into the NSF's vision of an "open knowledge network."`)]),at,t("p",null,[e("Finally, the file server hosts a folder containing lists of case numbers, which enumerate the cases in the above file collections as well as several other categories (e.g. the scraper runs described "),ot,e("). A full outline of these case lists can be found "),a(i,{to:"/rdf/#case-lists"},{default:s(()=>[e("here")]),_:1}),e(".")]),st,t("p",null,[e("Our DigitalOcean-hosted Postgres instance is essentially a staging area for data used in the Satyrn app. The original Satyrn development team, when deciding how to deliver data to Satyrn, designed a Postgres schema that they felt would be maximally interoperable with the app. We've adopted this schema with very few changes; despite the overhead of translating from our Mongo schema to this slightly different one, we've found that we can avoid even greater overhead by focusing on reducing friction between Satyrn's data source and the Satyrn app itself. (For an explanation of why we maintain two separate databases in the first place, see the section on "),a(i,{to:"/guide/pipeline/#mongodb-database"},{default:s(()=>[e("Mongo")]),_:1}),e(".)")]),nt,t("p",null,[t("em",null,[e("(n.b.: At time of writing, the Satyrn team is considering overhauling the app's data-ingestion functionality by substituting the "),a(i,{to:"/guide/pipeline/#file-server"},{default:s(()=>[e("aforementioned")]),_:1}),e(" SPARQL endpoint in place of our in-house Postgres endpoint. We plan on keeping this section as up-to-date as possible, but just in case, we suggest you check the satyrn-api repo to see if SPARQL compatibility has been added, e.g. by consulting the "),it,e(" switch blocks in the "),t("a",rt,[e("SQL-functions code"),a(o)]),e("). If SPARQL has been added, this section may be out of date.)")])]),dt,lt,t("p",null,[e("The Satyrn app consists of a React/Typescript frontend assisted by a small Postgres database for user data (both derived from the "),t("a",ct,[e("scales-ux"),a(o)]),e(" repo) and a Flask backend (from the "),t("a",ht,[e("satyrn-api"),a(o)]),e(" repo), all running on a DigitalOcean droplet at "),t("a",ut,[e("satyrn.scales-okn.org"),a(o)]),e('. To load data, the app uses a config file called a "ring," which must contain a database connection string (this is where we pass the IP of our Postgres instance) in addition to a JSON description of the tables in the database and the joins & other operations that can be performed on those tables. Changes to the data in the Postgres database propagate automatically into the app, but whenever there are changes to the structure of the database, the Satyrn dev team must manually update the ring file to reflect those changes.')]),pt,t("p",null,[e("More information about the Satyrn codebase can be found in the two repos linked above. As for questions about how to use the Satyrn app itself, we are in the process of implementing in-app tooltips to clarify Satyrn's features and capabilities. In the meantime, feel free to "),ft,e(" or contact the "),t("a",gt,[e("author"),a(o)]),e(" on GitHub.")]),t("div",mt,[a(r,{size:"1"},{default:s(()=>[e("⁵ Such behavior could theoretically look as follows: the word 'foo' was added to case X's Mongo HTML data, but that change hasn't reached Postgres yet, so despite case X's Postgres HTML data (as rendered in the Satyrn front-end) appearing to lack 'foo', a text search for 'foo' could match case X.")]),_:1})])])}const vt=l(k,[["render",yt],["__file","index.html.vue"]]);export{vt as default};
