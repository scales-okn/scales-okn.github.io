import{_ as r,M as l,p as c,q as p,R as n,N as a,V as o,t as e,a1 as t}from"./framework-5866ffd3.js";const u={},h=n("h1",{id:"scales-nlp",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#scales-nlp","aria-hidden":"true"},"#"),e(" SCALES NLP")],-1),m={style:{"margin-top":"-0.5em"}},k=n("em",null,"by Nathan Dahlberg â€¢ last updated Jan 9, 2024",-1),g=n("p",null,"An AI toolkit for legal research that includes a collection of deep learning models and utilities to:",-1),v=n("li",null,"Download dockets from PACER and parse their contents",-1),b=n("li",null,"Classify the text of docket entries with 70+ labels",-1),f=n("li",null,"Extract named entities from docket entries",-1),y=n("h2",{id:"pretrained-models",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#pretrained-models","aria-hidden":"true"},"#"),e(" Pretrained Models")],-1),_={href:"https://huggingface.co/scales-okn",target:"_blank",rel:"noopener noreferrer"},w=t(`<h3 id="getting-started" tabindex="-1"><a class="header-anchor" href="#getting-started" aria-hidden="true">#</a> Getting started</h3><p>To help illustrate how to use these models, we will use an example dataset.</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>entries = [
 &#39;MOTION by Plaintiff Charles Shulruff, D.D.S. to certify class (Edelman, Daniel) (Entered: 01/22/2016)&#39;,
 &#39;MOTION by Plaintiff Able Home Health, LLC to certify class (Edelman, Daniel) (Entered: 01/28/2016)&#39;,
 &#39;MOTION by Plaintiff Dr. Charles Shulruff, D.D.S. to certify class (Attachments: # 1 Exhibit A-E)(Edelman, Daniel) (Entered: 02/24/2016)&#39;,
 &#39;MOTION by Plaintiff Lindabeth Rivera to certify class (Carroll, Katrina) (Entered: 03/01/2016)&#39;,
 &#39;MOTION by Plaintiff Christy Griffith to certify class (Glapion, Jeremy) (Entered: 08/01/2016)&#39;
]

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="docket-language-model" tabindex="-1"><a class="header-anchor" href="#docket-language-model" aria-hidden="true">#</a> Docket Language Model</h3>`,4),x={href:"https://huggingface.co/scales-okn/docket-language-model",target:"_blank",rel:"noopener noreferrer"},E={href:"https://huggingface.co/microsoft/deberta-v3-large",target:"_blank",rel:"noopener noreferrer"},C=t(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># use with transformers</span>
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModel<span class="token punctuation">,</span> AutoTokenizer

model_name <span class="token operator">=</span> <span class="token string">&#39;scales-okn/docket-language-model&#39;</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>To use the model with the example data then we tokenize the entries and feed that as the input to the model.</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>model_inputs = tokenizer(entries, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
outputs = model(**model_inputs)
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>We can then access the calculated tensors for each of the five entries (output shape will be 5x49x1024).</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>outputs[0].shape
## torch.Size([5, 49, 1024])
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="docket-classifier" tabindex="-1"><a class="header-anchor" href="#docket-classifier" aria-hidden="true">#</a> Docket Classifier</h3>`,6),A={href:"https://huggingface.co/scales-okn/docket-classification",target:"_blank",rel:"noopener noreferrer"},S=t(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment">#Import the scales package</span>
<span class="token keyword">import</span> scales_nlp

<span class="token comment">#Load the model</span>
model <span class="token operator">=</span> scales_nlp<span class="token punctuation">.</span>pipeline<span class="token punctuation">(</span><span class="token string">&#39;docket-classifier&#39;</span><span class="token punctuation">)</span>

predictions <span class="token operator">=</span> model<span class="token punctuation">(</span>entries<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="docket-encoder" tabindex="-1"><a class="header-anchor" href="#docket-encoder" aria-hidden="true">#</a> Docket Encoder</h3>`,2),T={href:"https://huggingface.co/scales-okn/docket-encoder",target:"_blank",rel:"noopener noreferrer"},L={href:"https://www.sbert.net/docs/package_reference/losses.html#batchhardtripletloss",target:"_blank",rel:"noopener noreferrer"},D=t(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment">#Import the scales package</span>
<span class="token keyword">import</span> scales_nlp

<span class="token comment">#Load the model</span>
model <span class="token operator">=</span> scales_nlp<span class="token punctuation">.</span>pipeline<span class="token punctuation">(</span><span class="token string">&#39;docket-encoder&#39;</span><span class="token punctuation">)</span>

vectors <span class="token operator">=</span> model<span class="token punctuation">(</span>entries<span class="token punctuation">)</span>
vectors<span class="token punctuation">.</span>shape
<span class="token comment">## (5, 1024)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="quickstart" tabindex="-1"><a class="header-anchor" href="#quickstart" aria-hidden="true">#</a> Quickstart</h2><h3 id="installation" tabindex="-1"><a class="header-anchor" href="#installation" aria-hidden="true">#</a> Installation</h3><p>This module can be installed with pip:</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>$ pip <span class="token function">install</span> scales-nlp
</code></pre></div>`,5),P={href:"https://pytorch.org/get-started/locally/",target:"_blank",rel:"noopener noreferrer"},I=t(`<h3 id="configuration" tabindex="-1"><a class="header-anchor" href="#configuration" aria-hidden="true">#</a> Configuration</h3><p>To get the most out of this module, run the following to set module-wide configuration variables. These variables can also be set or overriden by your environment.</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>$ scales-nlp configure
</code></pre></div><ul><li><code>PACER_DIR</code> The folder where all of your PACER data will be saved and managed. This should be the same top-level directory that you use with the scraper and parser.</li><li><code>PACER_USERNAME</code> The username to your PACER account.</li><li><code>PACER_PASSWORD</code> The password to your PACER account.</li><li><code>HUGGING_FACE_TOKEN</code> You only need to include this if you want to use the pipelines API with private models on Hugging Face.</li></ul><p>In addition to these variables, you can also configure the default values that are used by the training routines API. These can be set by running <code>scales-nlp configure train-args</code>.</p><h2 id="collect-pacer-data" tabindex="-1"><a class="header-anchor" href="#collect-pacer-data" aria-hidden="true">#</a> Collect PACER Data</h2>`,6),R={href:"https://github.com/scales-okn/PACER-tools",target:"_blank",rel:"noopener noreferrer"},N=t(`<h3 id="simplified-scraper" tabindex="-1"><a class="header-anchor" href="#simplified-scraper" aria-hidden="true">#</a> Simplified Scraper</h3><p>To download a single case, provide a UCID to the following command:</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>$ scales-nlp download <span class="token punctuation">[</span>UCID<span class="token punctuation">]</span>
</code></pre></div><p>The UCID consists of the court abbreviation and the docket number, separated by two semicolons (e.g. <code>azd;;3:18-cv-08134</code>).</p><h3 id="simplified-parser" tabindex="-1"><a class="header-anchor" href="#simplified-parser" aria-hidden="true">#</a> Simplified Parser</h3><p>To run the parser across all of your downloaded cases, run the following:</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>$ scales-nlp parse
</code></pre></div><p>You may also provide a court abbreviation as an argument if you only want to apply the parser to cases within a single court.</p><h2 id="apply-scales-models" tabindex="-1"><a class="header-anchor" href="#apply-scales-models" aria-hidden="true">#</a> Apply SCALES Models</h2><h3 id="update-classifier-labels" tabindex="-1"><a class="header-anchor" href="#update-classifier-labels" aria-hidden="true">#</a> Update Classifier Labels</h3><p>The following command can be used to compute and update computed classifier labels from the SCALES litigation ontology to new data. By default the model outputs will be cached in your PACER_DIR and the model will only be applied to new cases that do not already have saved predictions. You can override saved predictions by passing the <code>--reset</code> flag. For optimal performance it is recommended that you only perform inference using the SCALES models on a device with a GPU. If you run into memory errors, try adjusting the <code>--batch-size</code> to your needs.</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>$ scales-nlp update-labels --batch-size <span class="token number">4</span>
</code></pre></div><h3 id="update-named-entity-extraction" tabindex="-1"><a class="header-anchor" href="#update-named-entity-extraction" aria-hidden="true">#</a> Update Named-entity Extraction</h3><p>SCALES will release several NER models in the near future.</p><h2 id="loading-tagged-dockets" tabindex="-1"><a class="header-anchor" href="#loading-tagged-dockets" aria-hidden="true">#</a> Loading Tagged Dockets</h2>`,15),O=n("code",null,"Docket",-1),M=n("code",null,"Docket",-1),z=t(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> scales_nlp

docket <span class="token operator">=</span> scales_nlp<span class="token punctuation">.</span>Docket<span class="token punctuation">.</span>from_ucid<span class="token punctuation">(</span><span class="token string">&quot;CASE UCID&quot;</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>docket<span class="token punctuation">.</span>ucid<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>docket<span class="token punctuation">.</span>case_name<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>docket<span class="token punctuation">.</span>header<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> entry <span class="token keyword">in</span> docket<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>entry<span class="token punctuation">.</span>row_number<span class="token punctuation">,</span> entry<span class="token punctuation">.</span>entry_number<span class="token punctuation">,</span> entry<span class="token punctuation">.</span>date_filed<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>entry<span class="token punctuation">.</span>text<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>entry<span class="token punctuation">.</span>event<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>entry<span class="token punctuation">.</span>labels<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>entry<span class="token punctuation">.</span>spans<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,1);function U(G,q){const d=l("font"),i=l("RouterLink"),s=l("ExternalLinkIcon");return c(),p("div",null,[h,n("div",m,[a(d,{size:"2"},{default:o(()=>[k]),_:1})]),g,n("ul",null,[v,b,f,n("li",null,[e("Apply a case-level logic to tagged data to identify key opening and dispositive events according to the "),a(i,{to:"/guide/ontology/"},{default:o(()=>[e("SCALES Litigation Ontology")]),_:1})])]),y,n("p",null,[e("This module builds on a collection of publicly available pretrained language models that can used to apply the SCALES Litigation Ontology to your own data. Check out the "),n("a",_,[e("scales-okn"),a(s)]),e(" page on the Hugging Face Model Hub for updates.")]),w,n("p",null,[n("a",x,[e("scales-okn/docket-language-model"),a(s)])]),n("p",null,[e("The base model that we use for downstream fine-tuning on both classification and entity recognition tasks. This model builds on a "),n("a",E,[e("microsoft/deberta-v3-large"),a(s)]),e(" that has been further finetuned on 11 million docket entries using the masked language modelling task.")]),C,n("p",null,[n("a",A,[e("scales-okn/docket-classification"),a(s)])]),n("p",null,[e("A multi-label classifier that has been finetuned to predict the SCALES litigation ontology labels. The complete list of labels and their descriptions can be found "),a(i,{to:"/guide/ontology/#classifier-labels"},{default:o(()=>[e("here")]),_:1})]),S,n("p",null,[n("a",T,[e("scales-okn/docket-encoder"),a(s)])]),n("p",null,[e("A text encoding model that can be used to map a docket entry to a vector for similarity search. This model was finetuned using semi-supervised training routine based on a contrastive, multi-dimensional version of "),n("a",L,[e("batch hard triplet loss"),a(s)]),e(" aimed at approximating the relative tfidf distances between batches of docket entries. Note that if using this model without the SCALES 'docket-encoder' pipeline, it is the CLF token embedding in the last hidden layer that is used as the embedding representation for the whole entry.")]),D,n("p",null,[e("You should also have the appropriate version of "),n("a",P,[e("PyTorch"),a(s)]),e(" installed for your system.")]),I,n("p",null,[e("This module includes simplified wrappers for the SCALES scraper and parser. This version eschews much of those tools' original functionality in order to make it easy to download a single case. For running bulk downloads of PACER data or otherwise taking advantage of the wide range of functionality available in the original implementation, you can consult the "),n("a",R,[e("PACER-tools"),a(s)]),e(" package, as well as the individual documentation pages for the "),a(i,{to:"/scraper/"},{default:o(()=>[e("scraper")]),_:1}),e(" and "),a(i,{to:"/parser/"},{default:o(()=>[e("parser")]),_:1}),e(".")]),N,n("p",null,[e("Downloaded cases can be loaded using the SCALES NLP "),O,e(" object as follows. If classifier labels or entity spans have been computed for the case, these will accessible as well. Furthermore, the "),M,e(" object will consolidate all of the information available in the case to infer the specific pathway events. To learn more about labels, entities, and pathway events, check out the "),a(i,{to:"/guide/ontology/"},{default:o(()=>[e("Litigation Ontology Guide")]),_:1}),e(".")]),z])}const H=r(u,[["render",U],["__file","index.html.vue"]]);export{H as default};
