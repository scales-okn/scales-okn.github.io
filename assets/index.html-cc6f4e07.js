import{_ as d,M as n,p as c,q as r,R as e,N as t,V as l,t as a,a1 as s}from"./framework-5866ffd3.js";const p={},u=e("h1",{id:"scraper",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#scraper","aria-hidden":"true"},"#"),a(" Scraper")],-1),h={style:{"margin-top":"-0.5em"}},m=e("em",null,"by Greg Mangan â€¢ last updated Mar 30, 2023",-1),f=s('<p>A collection of web scrapers to download data from Pacer.gov. The <code>scraper.py</code> script contains five scraper modules:</p><ol><li>Query Scraper</li><li>Docket Scraper</li><li>Summary Scraper</li><li>Member Scraper</li><li>Document Scraper</li></ol><table><thead><tr><th></th><th>Purpose</th><th>Input</th><th>Output</th></tr></thead><tbody><tr><td><em>Query Scraper</em></td><td>Pull case query results</td><td>Query parameters</td><td>Query results page (<em>html</em>)</td></tr><tr><td><em>Docket Scraper</em></td><td>Pull case dockets</td><td>Query html or csv</td><td>Case dockets (<em>html</em>)</td></tr><tr><td><em>Summary Scraper</em></td><td>Pull case summaries</td><td>Query html or csv</td><td>Case summaries (<em>html</em>)</td></tr><tr><td><em>Member Scraper</em></td><td>Pull MDL member case pages</td><td>Query html or csv</td><td>Member cases pages (<em>html</em>)</td></tr><tr><td><em>Document Scraper</em></td><td>Pull case documents &amp; attachments</td><td>Case dockets</td><td>Case documents (<em>pdf</em>)</td></tr></tbody></table><h2 id="getting-started" tabindex="-1"><a class="header-anchor" href="#getting-started" aria-hidden="true">#</a> Getting Started</h2><h3 id="setup" tabindex="-1"><a class="header-anchor" href="#setup" aria-hidden="true">#</a> Setup</h3><p>To run this scraper you will need the following:</p>',6),v=e("li",null,"Python 3.7+",-1),g={href:"https://selenium-python.readthedocs.io/index.html",target:"_blank",rel:"noopener noreferrer"},k={href:"https://www.mozilla.org/en-US/firefox/new/",target:"_blank",rel:"noopener noreferrer"},b={href:"https://github.com/mozilla/geckodriver",target:"_blank",rel:"noopener noreferrer"},w=s(`<h3 id="login-details" tabindex="-1"><a class="header-anchor" href="#login-details" aria-hidden="true">#</a> Login Details</h3><p>Before running the scraper, you will need to have an account on <a href="Pacer.gov">Pacer.gov</a>. You will need to create an auth file (in .json format) with your login details as below:</p><div class="language-json" data-ext="json"><pre class="language-json"><code><span class="token punctuation">{</span>
    <span class="token property">&quot;user&quot;</span><span class="token operator">:</span> <span class="token string">&quot;&lt;your_username&gt;&quot;</span><span class="token punctuation">,</span>
    <span class="token property">&quot;pass&quot;</span><span class="token operator">:</span> <span class="token string">&quot;&lt;your_password&gt;&quot;</span>
<span class="token punctuation">}</span>
</code></pre></div><h3 id="directory-structure" tabindex="-1"><a class="header-anchor" href="#directory-structure" aria-hidden="true">#</a> Directory Structure</h3><p>As the scraper is run on a single district court at a time, it is recommended that Pacer downloads should be separated into different directories by court. An example of a data folder is the following:</p><div class="language-text" data-ext="text"><pre class="language-text"><code>/data
|-- pacer
|    |-- ilnd
|    |-- nyed
|    |-- txsd
|    |-- ...
</code></pre></div><p>When running the scraper, a court directory will have an imposed structure as below (the necessary sub-directories will be created):</p><div class="language-text" data-ext="text"><pre class="language-text"><code>/ilnd
|-- html            # Orginal case dockets
|   |-- 1-16-cv-00001.html
|   |-- ...
|   
|-- json            # Parsed case dockets
|   |-- 1-16-cv-00001.json
|   |-- ...
|
|-- queries         # Downloaded queries and saved configs
|   |-- 2016cv_result.html
|   |-- 2016cv_config.json
|   |-- ...
|
|-- summaries       # Downloaded case summaries
|   |-- 1-16-cv-00001.html
|   |-- ...
|
|-- docs            # Downloaded documents and attachments
|   |-- ilnd;;1-16-cv-00001_1_2_u7905a347_t200916.pdf
|   |-- ...
|
|-- _temp_          # Temporary download folder for scraper
|   |-- 0
|       | ...
|   |-- 1
|       | ...
|   |-- ...
</code></pre></div><h3 id="ucids-unique-case-identifiers" tabindex="-1"><a class="header-anchor" href="#ucids-unique-case-identifiers" aria-hidden="true">#</a> UCIDs (unique case identifiers)</h3><p>To uniquely identify cases, the scraper uses its own identifier called UCIDs, which are constructed from the following two components:</p><div class="language-text" data-ext="text"><pre class="language-text"><code>&lt;court abbreviation&gt;;;&lt;case id&gt;
</code></pre></div><p>For example, the case <code>1:16-cv-00001</code> in the Northern District of Illinois would be identified as <code>ilnd;;1:16-cv-00001</code>.</p><p><em>Note: In some districts, it is common to include judge initials at the end of a case id, e.g. <code>2:15-cr-11112-ABC-DE</code>. These initials are always excluded from the UCID</em>.</p><h3 id="runtime" tabindex="-1"><a class="header-anchor" href="#runtime" aria-hidden="true">#</a> Runtime</h3><p>The scraper is designed to run at night to reduce its impact on server load. By default it will only run between 8pm and 4am (CDT). These parameters can be altered and overridden through the <code>-rts,</code> <code>-rte</code> and <code>--override-time</code> options (see below for details).</p><h3 id="fees" tabindex="-1"><a class="header-anchor" href="#fees" aria-hidden="true">#</a> Fees</h3><p>Pacer fees can rack up quickly! Running this scraper will incur costs to your own Pacer account. There are a number of options for the scraper that exist to limit the potential for accidentally incurring large charges:</p><ul><li>Docket limit - A maximum no. of dockets to be downloaded can be specified (see <code>--docket-limit</code> below).</li><li>Document limit - A maximum can be specified so as to exclude certain dockets from the Document Scraper that have large amounts of documents (see <code>--document-limit</code> below).</li></ul><h2 id="usage" tabindex="-1"><a class="header-anchor" href="#usage" aria-hidden="true">#</a> Usage</h2><p>To run the scraper:</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>python scrapers.py <span class="token punctuation">[</span>OPTIONS<span class="token punctuation">]</span> INPATH
</code></pre></div><h3 id="arguments" tabindex="-1"><a class="header-anchor" href="#arguments" aria-hidden="true">#</a> Arguments</h3><ul><li><code>inpath</code>: Relative path to the court directory folder, e.g. <code>../../data/pacer/ilnd</code>. This is the directory that will have the imposed structure as outlined above.</li></ul><h3 id="options" tabindex="-1"><a class="header-anchor" href="#options" aria-hidden="true">#</a> Options</h3><p>The options passed to the scraper can be grouped into the following four categories:</p><p><em>General</em> <em>(apply to all three modules)</em></p><ul><li><p><code>-m, --mode</code> <em>[query|docket|summary|member|document]</em> Which scraper mode to run.</p></li><li><p><code>-a, --auth-path PATH</code> Relative path to login details auth file (see above).</p></li><li><p><code>-c, --court</code> The standard abbreviation for district court being scraped, e.g. <code>ilnd</code>.</p></li><li><p><code>-nw, --n-workers INTEGER</code> No. of workers to run simultaneously (for docket/document scrapers), i.e. no. of simultaneous browsers running.</p></li><li><p><code>-ct, --case-type TEXT</code> Specify a single case type to filter query results. If none given, scraper will pull &#39;<em>cv</em>&#39; and &#39;<em>cr</em>&#39; cases.</p></li><li><p><code>-rts, --runtime-start INTEGER</code> <em>(default:20)</em> The start runtime hour (in 24hr, CDT). The scraper will not run if the current hour is before this hour.</p></li><li><p><code>-rte, --runtime-end INTEGER</code> <em>(default:4)</em> The end runtime hour (in 24hr, CDT). The scraper stop running when the current hour reaches this hour.</p></li><li><p><code>--override-time</code> Override the time restrictions and run scraper regardless of current time.</p></li><li><p><code>--case-limit INTEGER</code> Sets limit on maximum no. of cases to process (enter &#39;false&#39; or &#39;f&#39; for no limit). This will be applied as a limit on: - the no. of case dockets the docket scraper pulls - the no. of case dockets the document scraper takes as an input</p></li><li><p><code>--headless</code> Run Selenium in headless mode (i.e. no Firefox window will appear); useful if running on a server that does not have a display.</p></li><li><p><code>--verbose</code> Give slightly more verbose logging output.</p></li></ul><p><em>Query Scraper</em></p><ul><li><p><code>-qc, --query-conf PATH</code> Configuration file (.json) for the query that will be used to populate the query form on Pacer. If none is specified, the query builder will run in the terminal.</p></li><li><p><code>--query-prefix TEXT</code> A prefix for the filenames of output query HTMLs. If the date range of the query is greater than 180 days, the query will be split into chunks of 31 days to prevent PACER crashing while serving a large query results page. Multiple files will be created that follow the pattern <code>{query_prefix}__i.html</code>, where <code>i</code> enumerates over the date range chunks.</p></li></ul><p><em>Docket Scraper</em></p><ul><li><p><code>--docket-input PATH</code> A relative path that is the input for the Docket Scraper module: this can be a single query result page (.html), a directory of query html files or a csv with UCIDs</p></li><li><p><code>-mem, --docket-mem-list</code> <em>[always|avoid|never] (default: never)</em> How to deal with member lists in docket reports (affects costs particularly with class actions and MDLs)</p><ul><li><code>always</code>: Always include them in reports</li><li><code>avoid</code>: Do not include them in a report if the current case was previously seen listed as a member case in a previously downloaded docket</li><li><code>never</code>: Never include them in reports</li></ul></li><li><p><code>--docket-exclude-parties</code> If True, &#39;Parties and counsel&#39; and &#39;Terminated parties&#39; will be excluded from docket reports (this reduces the page count for the docket report so can reduce costs).</p></li><li><p><code>-ex, --docket-exclusions PATH</code> Relative path to a csv file with a column of UCIDs that are cases to be excluded from the Docket Scraper.</p></li><li><p><code>--docket-update</code> Check for new docket lines in existing cases. A <code>--docket-input</code> must also be provided. If the docket input is a csv, a <code>latest_date</code> column <em>can</em> be provided to give the latest date across docket lines for each case. This date (+1) is passed to the &quot;date filed from&quot; field in Pacer when the docket report is generated. If no <code>latest_date</code> column provided for a case that has been previously downloaded, the date is calculated from the case json.</p></li></ul><p><em>Summary Scraper</em></p><ul><li><code>--summary-input PATH</code> Similar to <code>--docket-input</code>. A relative path that is the input for the Summary Scraper module: this can be a single query result page (.html), a directory of query html files or a csv with UCIDs.</li></ul><p><em>Member Scraper</em></p><ul><li><code>--member-input PATH</code> A relative path to a csv that has at least one of the following columns: <em>pacer_id, case_no, ucid</em></li></ul><p><em>Document Scraper</em></p><ul><li><p><code>--document-input PATH</code> A relative path that is the Document Scraper module: a csv file that contains a <em>ucid</em> column. These will be the cases that the Document Scraper will run on. If a <em>doc_no</em> column is provided, then the specific cases specified will be downloaded, see <a href="#downloading-specific-documents">Downloading specific documents</a> below. Otherwise, an error will appear warning the user to use the --document-all-docs option, if they want to download all documents for a case (see below).</p></li><li><p><code>--document-all-docs</code> This will force the scraper to download <strong>all</strong> documents for each of the cases supplied in <em>document-input</em>. Warning: this can be very expensive!</p></li><li><p><code>--document-att / --no-document-att</code> <em>(default: True)</em> Whether or not to get document attachments from docket lines.</p></li><li><p><code>--document-skip-seen / --no-document-skip-seen</code> <em>(default:True)</em> Whether to skip seen cases. If true, documents will only be downloaded for cases that have not previously had documents downloaded. That is, if <code>CaseA</code> is in the input for the Document Scraper, it will be excluded and not have any documents downloaded in this session if there are any documents associated with <code>CaseA</code> that have previously been downloaded (i.e. that are in the <em>/docs</em> subdirectory).</p></li><li><p><code>--document-limit INTEGER</code> <em>(default: 1000)</em> A limit on the no. of documents <strong>within</strong> a case. Cases that have more documents that the limit (i.e. extremely long dockets) will be excluded from the Document Scraper step.</p></li></ul><h3 id="notes" tabindex="-1"><a class="header-anchor" href="#notes" aria-hidden="true">#</a> Notes</h3><h4 id="downloading-specific-documents" tabindex="-1"><a class="header-anchor" href="#downloading-specific-documents" aria-hidden="true">#</a> Downloading specific documents</h4><p>When giving the Document Scraper specific dockets to download, you can specify specific documents to download from each docket. If you need to download <strong>every</strong> document in each case you have supplied, then you need to use the <code>--document-all-docs</code> flag.</p><p>There are two types of documents that can be downloaded:</p><ol><li>Line documents: these are documents that relate to the whole docket entry line in the docket report; the links for these documents appear in the # column of the docket report table.</li><li>Attachments: these are attachments or exhibits included in the line; they are referenced in-line in the docket entry text.</li></ol><p><em>Note: Many docket entries contain links with references to documents from previous lines. These are ignored and not treated as attachments. To download these, refer to their original line.</em></p><p>To specify specific documents to be downloaded, give the <code>--document-input</code> argument a csv that has both a <em>ucid</em> and a <em>doc_no</em> column. The <em>doc_no</em> column is a column where you can give a comma delimited list of documents to download. The following are valid individual values:</p><ul><li><em>x</em> - just the line document <em>x</em></li><li><em>x:y</em> - the line documents from <em>x</em> to <em>y</em>, inclusive</li><li><em>x_z</em> - the <em>z</em>&#39;th attachment on line <em>x</em></li><li><em>x_a:b</em> - attachments <em>a</em> through <em>b</em>, inclusive, from line <em>x</em></li></ul><p>These values are combined into a comma-delimited list; for example, for a given case you could specify: <em>&quot;2,3:5,6_1,7_1:4&quot;</em>. (See &quot;Common Tasks&quot; below for a full example of this.)</p><p>Notes:</p><ul><li>If <em>doc_no</em> column is <strong>not</strong> present in the csv and the <code>--document-all-docs</code> flag has not been supplied, the scraper will give an error message. You need to either supply a <code>doc_no</code> column or specify that you want to download all documents for each case, by using the <code>--document-all-docs</code> flag.</li><li>If <em>doc_no</em> column <strong>is</strong> present and there is a row with a case that has no value (empty string) specified for doc_no, <strong>all</strong> documents will be downloaded for that case. Note: this may be very expensive.</li><li>The no. or index of the document corresponds to the # column in the docket table on PACER. These are not necessarily displayed in sequential order due to PACER filing peculiarities.</li></ul><h4 id="specific-defendant-dockets" tabindex="-1"><a class="header-anchor" href="#specific-defendant-dockets" aria-hidden="true">#</a> Specific defendant dockets</h4><p>For criminal cases, there may be separate dockets/stubs for defendants if there are multiple defendants. To download a docket for a specific defendant, you can supply a <code>def_no</code> column in the docket input csv. In this column, any blank value will be interpreted as getting the main docket. If the <code>def_no</code> column is excluded, the scraper will pull the main docket for every case.</p><p>For example, given the following file:</p><p><em>/docket_update.csv</em></p><div class="language-csv line-numbers-mode" data-ext="csv"><pre class="language-csv"><code><span class="token value">ucid</span><span class="token punctuation">,</span><span class="token value">def_no</span>
<span class="token value">ilnd;;1:16-cr-12345</span><span class="token punctuation">,</span><span class="token value">2</span>
<span class="token value">ilnd;;1:16-cr-12345</span><span class="token punctuation">,</span><span class="token value">3</span>
<span class="token value">ilnd;;1:16-cr-12346</span><span class="token punctuation">,</span>
<span class="token value">ilnd;;1:16-cr-12347</span><span class="token punctuation">,</span><span class="token value">4</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Running the following</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>python scrapers.py <span class="token parameter variable">-m</span> docket
  --docket-input <span class="token operator">&lt;</span>path_to_file<span class="token operator">&gt;</span>/docket_update.csv --docket-update <span class="token operator">&lt;</span>path_to_ilnd_folder<span class="token operator">&gt;</span>
</code></pre></div><p>will pull the following dockets:</p><ul><li><em>ilnd;;1:16-cr-12345</em>: the docket for defendants 2 and 3</li><li><em>ilnd;;1:16-cr-12346</em>: the main docket</li><li><em>ilnd;;1:16-cr-12347</em>: the docket for defendant 4</li></ul><h3 id="common-tasks" tabindex="-1"><a class="header-anchor" href="#common-tasks" aria-hidden="true">#</a> Common tasks</h3><h4 id="_1-run-a-search-query-from-start-to-finish" tabindex="-1"><a class="header-anchor" href="#_1-run-a-search-query-from-start-to-finish" aria-hidden="true">#</a> 1. Run a search query from start to finish</h4><p>Suppose you want to run a search query -- for example, all cases opened in Northern Illinois in the first week of 2020. To do this:</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>python scrapers.py <span class="token parameter variable">-m</span> query <span class="token parameter variable">-a</span> <span class="token operator">&lt;</span>path_to_auth_file<span class="token operator">&gt;</span> --query-prefix <span class="token string">&quot;first_week_2020&quot;</span>
   <span class="token parameter variable">-c</span> ilnd <span class="token operator">&lt;</span>path_to_ilnd_folder<span class="token operator">&gt;</span>
</code></pre></div><p>Since the Query Scraper module will run and no query config file has been specified, the query config builder will run in the terminal, allowing you to enter search parameters for the Pacer query form.</p><h4 id="_2-downloading-dockets" tabindex="-1"><a class="header-anchor" href="#_2-downloading-dockets" aria-hidden="true">#</a> 2. Downloading Dockets</h4><p>Suppose you have already run the above search query, and it created a file at <code>pacer/ilnd/queries/first_week_2020.html</code>. Now, to download all civil and criminal cases included in that search result, run the following:</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>python scrapers.py <span class="token parameter variable">-m</span> docket <span class="token parameter variable">-a</span> <span class="token operator">&lt;</span>path_to_auth_file<span class="token operator">&gt;</span>
   --document-input <span class="token operator">&lt;</span>path_to_first_week_2020.html<span class="token operator">&gt;</span>
   <span class="token parameter variable">-c</span> ilnd <span class="token operator">&lt;</span>path_to_ilnd_folder<span class="token operator">&gt;</span>
</code></pre></div><p>The dockets will be downloaded into <code>pacer/ilnd/html/&lt;year&gt;/html/</code>, depending on the year code in the case id. Note that this may differ from the actual filing date (e.g. a case <code>ilnd;;1:20-cv-XXXX</code> may have a filing date from 2019 in PACER).</p><p>Alternatively, if you had the list of cases (either from that query html file or just an ad-hoc/manual list), you could put them in a csv file (with a <code>ucid</code> column) and pass that as the argument in for <code>--document-input</code> instead of the query html file.</p><h4 id="_3-run-document-scraper-on-a-subset-of-dockets" tabindex="-1"><a class="header-anchor" href="#_3-run-document-scraper-on-a-subset-of-dockets" aria-hidden="true">#</a> 3. Run Document Scraper on a subset of dockets</h4><p>If you have have previously downloaded a bunch of case dockets and you want to download the documents for just a subset of these cases, you first need to create a file with the subset of interest. This can be any csv file that has a UCID column and a doc_no column, which we will create and call <em>subset.csv</em>, as below:</p><div class="language-csv line-numbers-mode" data-ext="csv"><pre class="language-csv"><code><span class="token value">ucid</span><span class="token punctuation">,</span><span class="token value">doc_no</span>
<span class="token value">ilnd;;1:16-cv-03630</span><span class="token punctuation">,</span><span class="token value">2</span>
<span class="token value">ilnd;;1:16-cv-03631</span><span class="token punctuation">,</span><span class="token value">&quot;4,5&quot;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>To run the document scraper on just this subset you could do the following:</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>python scraper.py <span class="token parameter variable">-m</span> document
  <span class="token parameter variable">-a</span> <span class="token operator">&lt;</span>path_to_auth_file<span class="token operator">&gt;</span> <span class="token parameter variable">-c</span> ilnd
  --document-input <span class="token operator">&lt;</span>path_to_subset.csv<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>path_to_ilnd_folder<span class="token operator">&gt;</span>
</code></pre></div><p><em>Notes:</em></p><ul><li><em>The dockets for these cases must have been downloaded and must be in the /html folder for the Document Scraper to detect them.</em></li><li><em>The <code>doc_no</code> column will download specific documents (see more in <a href="#download-specific-documents">Download specific documents</a> below)</em></li><li><em>If you need to download all documents in each case, you can forgo the <code>doc_no</code> column and supply the <code>--document-all-docs</code> flag (see above)</em>.</li></ul><h4 id="_4-update-dockets" tabindex="-1"><a class="header-anchor" href="#_4-update-dockets" aria-hidden="true">#</a> 4. Update dockets</h4><p>To run a docket update, you need to give a csv file to the <code>--docket-input</code> argument and also use the <code>--docket-update</code> flag, as in the following csv:</p><p><em>/docket_update.csv</em></p><div class="language-csv line-numbers-mode" data-ext="csv"><pre class="language-csv"><code><span class="token value">ucid</span><span class="token punctuation">,</span><span class="token value">latest_date</span>
<span class="token value">ilnd;;1:16-cv-03630</span><span class="token punctuation">,</span><span class="token value">1/31/2016</span>
<span class="token value">ilnd;;1:16-cv-03631</span>
<span class="token value">ilnd;;1:16-cv-03632</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>To run the scraper:</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>python scrapers.py <span class="token parameter variable">-m</span> docket
  --docket-input <span class="token operator">&lt;</span>path_to_file<span class="token operator">&gt;</span>/docket_update.csv
  --docket-update <span class="token operator">&lt;</span>path_to_ilnd_folder<span class="token operator">&gt;</span>
</code></pre></div><p>Suppose that ..630 and ..631 are cases that have previously been downloaded, but ...632 has not been downloaded yet. The following will occur when the Docket Scraper runs:</p><ul><li>For ..630: the date 2/1/2016 will be passed to the date_from field in Pacer when the docket report is generated. A new docket will be downloaded and saved as ..630_1.html (or _2, _3 etc depending on if previous updates exist).</li><li>For ..631: as it has previously been downloaded but no date has been given in the <em>latest_date</em> column, the date of the latest docket entry will be retrieved from the case json and filled in as the<em>latest_date</em>, the rest proceeds as above</li><li>For ...632: since this case has not previously been downloaded, the whole docket report will be downloaded (i.e. it will proceed as normal for this case)</li></ul><h4 id="_5-download-specific-documents" tabindex="-1"><a class="header-anchor" href="#_5-download-specific-documents" aria-hidden="true">#</a> 5. Download specific documents</h4><p>When running the Document Scraper, you can specify a list of specific documents to download (see above for valid values). For example, suppose the following file is given:</p><p><em>document_downloads.csv</em></p><div class="language-csv line-numbers-mode" data-ext="csv"><pre class="language-csv"><code><span class="token value">ucid</span><span class="token punctuation">,</span><span class="token value">doc_no</span>
<span class="token value">ilnd;;1:16-cv-03630</span><span class="token punctuation">,</span><span class="token value">&quot;1,3:5&quot;</span>
<span class="token value">ilnd;;1:16-cv-03631</span><span class="token punctuation">,</span><span class="token value">&quot;7_6, 7_9:11,&quot;</span>
<span class="token value">ilnd;;1:16-cv-03632</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>To run this</p><div class="language-bash" data-ext="sh"><pre class="language-bash"><code>python scrapers.py <span class="token parameter variable">-m</span> document
  --document-input <span class="token operator">&lt;</span>path_to_file<span class="token operator">&gt;</span>/document_downloads.csv
  <span class="token operator">&lt;</span>path_to_ilnd_folder<span class="token operator">&gt;</span>
</code></pre></div><p>When it runs, the document downloader will download the following:</p><ul><li>For case ...630: line documents 1, 3, 4, and 5</li><li>For case ...631: attachments 6, 9, 10, and 11 from line 7</li><li>For case ...632: all documents</li></ul>`,90);function y(_,x){const i=n("font"),o=n("ExternalLinkIcon");return c(),r("div",null,[u,e("div",h,[t(i,{size:"2"},{default:l(()=>[m]),_:1})]),f,e("ul",null,[v,e("li",null,[e("a",g,[a("Selenium"),t(o)]),a(" 3.12+")]),e("li",null,[e("a",k,[a("Firefox"),t(o)]),a(" 80.0+")]),e("li",null,[e("a",b,[a("GeckoDriver"),t(o)])])]),w])}const q=d(p,[["render",y],["__file","index.html.vue"]]);export{q as default};
